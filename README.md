# Sauria Systolic Array Accelerator – Verification Environment

UVM-based functional verification of a systolic-array–based compute accelerator, with focus on tile management, dataflow orchestration, compute sequencing, and array-level execution behavior.

This project uses the original Sauria architecture and RTL strictly as the design under test. All other components of the upstream repository, including any software stack, testing framework, or system-level integration harness, are intentionally excluded.

The **goal** is to develop a reusable, IP-level verification environment capable of validating correct tiled execution behavior and control-driven compute sequencing, while remaining narrowly scoped to the architectural blocks that define correct operation.

The **environment** is structured to reflect production-quality UVM practices, including modular agents, targeted stimulus, functional coverage, and assertion-based checking.

The **verification architecture** is intentionally modular, allowing incremental expansion or integration without requiring structural redesign.

---

## Out of Scope

- **DMA Engine and Memory Subsystem**
Verification of the DMA engine implementation, memory hierarchy behavior, and external memory interfaces is excluded. The DMA is treated as a consumer of tile-level metadata generated by the dataflow controller, and as a producer of data visible to the compute core, without attempting to validate memory correctness, performance, or protocol behavior.

- **Data Feeder**
The data feeder module is excluded from verification. The data feeder performs on-the-fly convolution lowering (e.g., im2col-style transformation) to generate streamed input data for the systolic array. This functionality introduces convolution-specific data transformation semantics that are orthogonal to tile management, control sequencing, and array execution correctness. Excluding the data feeder allows the verification effort to remain focused on validating control-driven tiled execution rather than data layout transformation logic.

- **Weight Fetcher**
The weight fetcher and associated control logic are excluded. Weight movement and buffering semantics are treated as correct by construction, allowing the verification effort to focus on tile-level orchestration and array execution behavior.

- **Partial Sums Manager**
Logic responsible for managing partial sums outside of the systolic array datapath is excluded. Verification of accumulation correctness is limited to behavior observable within the array itself.

- **Local SRAMs**
On-chip SRAM structures used for buffering activations, weights, or partial results are excluded from verification. These memories are treated as ideal storage elements without attempting to validate internal memory behavior or arbitration logic.

---

## Verification Strategy

The verification strategy decomposes accelerator execution risk into independent architectural concerns, enabling focused validation while preserving overall correctness.

**Stimulus** generation combines directed and constrained-random sequences targeting specific tile management scenarios, control transitions, and compute behaviors. Configuration transactions describe tensor dimensions, tile geometry, and tiling progression, and are delivered to the dataflow controller to initiate execution.

**Checking** is distributed across domain-specific scoreboards covering tile sequencing, compute-core control behavior, and array-level execution results. Assertion-based checks are used to enforce temporal invariants, control handshakes, and completion conditions.

Where memory interaction is required for stimulus or observability, abstracted interfaces are used to provide deterministic data delivery and visibility, without modeling a full memory subsystem.

**Functional coverage** is intent-driven and used to measure exploration of tile boundaries, configuration combinations, control state transitions, and compute completion scenarios. Coverage is used to guide stimulus refinement rather than as a standalone metric.

External interfaces (AXI4-Lite for configuration and AXI4 for data movement) are implemented using reusable, externally sourced components that are treated as known-good infrastructure. These components are instantiated to enable realistic control and data movement but are not primary targets of verification in this environment.

Architectural intent is inferred from RTL structure and observable behavior. In the absence of a complete standalone specification, verification correctness is established through consistent interpretation of control semantics, tiling behavior, and compute outcomes.

The environment is modular by construction, allowing verification scope to scale through configuration and composition.

---

## Architecture

### Control and Execution Overview

Sauria is configured, controlled, and exchanges data primarily through AXI-based interfaces. AXI4-Lite is used for configuration and control register access, while AXI4 is used for bulk data movement. These interfaces form the primary mechanism for communication both into the Sauria subsystem and between major architectural blocks within the design.

Sauria is organized around a systolic array compute core controlled through a two-level control hierarchy:

* A **top-level dataflow controller** responsible for tensor-level configuration, tile management, and operation launch
* A **compute-core controller** responsible for sequencing compute phases within the core

The dataflow controller holds tensor dimensions, tile geometry, and tiling parameters, and generates per-tile metadata that is provided to the DMA engine for loading data into on-chip memories. Once execution is launched, the DMA engine and compute core operate independently.

The compute-core controller sequences array execution for each tile. Upon completion of the programmed operation, the compute core signals completion back to the dataflow controller.

### Verification Configuration (Instantiated Blocks)

The verification environment instantiates a complete Sauria subsystem, including all RTL blocks that form part of the accelerator architecture. While verification focus is selectively applied, the full subsystem is present to preserve realistic connectivity and control interactions.

At a high level, the instantiated blocks include:

* Top-level dataflow controller
* DMA engine
* Compute core, including:

  * Systolic array
  * Compute-core controller
  * Data feeder
  * Weight fetcher
  * Partial sums manager
  * Local SRAMs and associated local control logic

Blocks listed as out of scope are instantiated but treated as functionally abstracted or assumed-correct components, providing stimulus delivery and observability without being the primary targets of verification.

Refer to project documentation for detailed microarchitectural information as it becomes available.

---

## Repository Structure
- `/pulp-platform` : Externally sourced reusable infrastructure and interface components
- `/RTL`       : Snapshot of Sauria RTL used for verification
- `/tb`        : Top-level testbench and UVM environment
- `/tests`     : Directed and constrained-random tests
- `/docs`      : Sauria diagram, Verification plan, architectural notes, and findings
- `/output`    : Compilation logs
- `/test_runs` : Test execution logs

---

## Quick Start

### Pre-Requisites

(To be documented)

### Build and Run

(To be documented)

Detailed setup and usage instructions will be added as the project progresses.

---
**Output accumulation for one tile:**

C<sub>tile</sub>[x,y,k] =
∑<sub>c_tile</sub> ( I<sub>tile</sub>[x,y,c_tile] · W<sub>tile</sub>[c_tile,k] )

---
Δ<sub>t</sub>[x,y,k] = I<sub>tile</sub>[x,y,c<sub>t</sub>] · W<sub>tile</sub>[c<sub>t</sub>,k]

---
P<sub>t+1</sub>[x,y,k] = P<sub>t</sub>[x,y,k] + Δ<sub>t</sub>[x,y,k]

---
P<sub>final</sub>[x<sub>tile</sub>,y<sub>tile</sub>,k<sub>tile</sub>] =
∑<sub>c_tile</sub> ( I<sub>tile</sub>[x<sub>tile</sub>,y<sub>tile</sub>,c<sub>tile</sub>] ·
W<sub>tile</sub>[c<sub>tile</sub>,k<sub>tile</sub>] )

---
P[x,y,k] = ∑<sub>c_tile</sub> ( ∑<sub>c∈c_tile</sub> I[x,y,c] · W[c,k] )

---
Generic GEMM:     C[m,n] = ∑<sub>k</sub> A[m,k] · B[k,n]

Sauria view:      P[x,y,k] = ∑<sub>c</sub> I[x,y,c] · W[c,k]


Generic (tiled): C[m,n] = ∑<sub>k_tile</sub> ( ∑<sub>k∈k_tile</sub> A[m,k]·B[k,n] )

Sauria (tiled):  P[x,y,k] = ∑<sub>c_tile</sub> ( ∑<sub>c∈c_tile</sub> I[x,y,c]·W[c,k] )

Generic dimension  →  Sauria dimension
-------------------   ----------------
m (rows of A)       →  (x, y) spatial tile index  
k (reduction dim)   →  c  (channel / reduction dimension)  
n (columns of B)    →  k  (output feature dimension)
C[m,n]              →  P[x,y,k]


Conceptually, Sauria is performing a GEMM where the row index m is split into 2D spatial tiles (x,y), the reduction dimension k becomes the channel dimension c, and the output dimension n maps directly to Sauria’s output feature dimension k.

[A]        × [B]        = [C]
[m × k]      [k × n]      [m × n]

[I]        × [W]        = [P]
[x,y × c]    [c × k]      [x,y × k]

--
## Sauria Repository

This project is based on a fork of the original Sauria repository:
https://github.com/bsc-loca/sauria
